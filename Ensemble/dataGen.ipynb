{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AVR12.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwN-2o2AYdeF"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, models, transforms\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "import glob\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_dX5V-tQn7F"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense,InputLayer, Conv2D, Conv2DTranspose, Flatten, Dropout, BatchNormalization, LeakyReLU, Input, MaxPool2D\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
        "#from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "#from tensorflow.keras.optimizers import Adam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOOPvbc8dCiG"
      },
      "source": [
        "basePath = '/content/'\n",
        "\n",
        "modelSave = basePath+'weights.pth'\n",
        "\n",
        "data_dir = basePath+'Sample'\n",
        "#data_dir = basePath+'Dataset_2'\n",
        "\n",
        "batch_size = 2\n",
        "num_epochs = 1\n",
        "input_size = 224\n",
        "NUM_CLASSES = 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkLw-FfnlRQM"
      },
      "source": [
        "image_types = [\"Covid-19\",\"No_findings\",\"Pneumonia\"]\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eP4d1ruvwBvA"
      },
      "source": [
        "def resize_images(image_dir):\n",
        "  for im_type in image_types:\n",
        "    # Iterate through each image file in each image_type folder\n",
        "    #  glob reads in any image with the extension \"image_dir/im_type/*\"\n",
        "    for file in glob.glob(os.path.join(image_dir, im_type, \"*\")):\n",
        "        im = Image.open(file)\n",
        "        f, e = os.path.splitext(file)\n",
        "        imResize = im.resize((input_size,input_size), Image.ANTIALIAS)\n",
        "        os.remove(file)\n",
        "        imResize.save(f + '.png', 'PNG', quality=90)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDlbcP3hwDvA",
        "outputId": "919627de-a81e-4de7-d8ae-c78514e6a10f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "'''!cp -R '/content/drive/My Drive/COVID-19/Sample' '/content/'\n",
        "#!cp -R '/content/drive/My Drive/COVID-19/Dataset_2' '/content/'\n",
        "resize_images(data_dir+'/train')\n",
        "resize_images(data_dir+'/val')'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"!cp -R '/content/drive/My Drive/COVID-19/Sample' '/content/'\\n#!cp -R '/content/drive/My Drive/COVID-19/Dataset_2' '/content/'\\nresize_images(data_dir+'/train')\\nresize_images(data_dir+'/val')\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2c78pEbWIko",
        "outputId": "749337c8-0f33-49cd-82ff-51279b897d4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "'''resize_images(data_dir+'/train')\n",
        "resize_images(data_dir+'/val')'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"resize_images(data_dir+'/train')\\nresize_images(data_dir+'/val')\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbHR5M_XwHOo"
      },
      "source": [
        "#!rm -R 'Sample'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRjriXBpYxrQ"
      },
      "source": [
        "class DenseNet121(nn.Module):\n",
        "    def __init__(self, out_size):\n",
        "        super(DenseNet121, self).__init__()\n",
        "        self.densenet121 = models.densenet121(pretrained=True)\n",
        "        num_ftrs = self.densenet121.classifier.in_features\n",
        "        self.densenet121.classifier = nn.Sequential(\n",
        "            nn.Linear(num_ftrs, out_size),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.densenet121(x)\n",
        "        return x\n",
        "    \n",
        "\n",
        "class CovidAID(DenseNet121):\n",
        "    def __init__(self):\n",
        "        #NUM_CLASSES = 3\n",
        "        super(CovidAID, self).__init__(NUM_CLASSES)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3HSUzDDPztV"
      },
      "source": [
        "def conv_block(out_channels, filter_dim=3, stride=1):\n",
        "    # for_pad = lambda s: s if s > 2 else 3\n",
        "    block = Sequential()\n",
        "    '''if filter_dim == 1:\n",
        "      block.add(Conv2DTranspose(out_channels,3, strides=(1,1), padding='valid', activation=None))\n",
        "    else:\n",
        "      block.add(Conv2D(out_channels, filter_dim, strides=(1,1), padding='same', activation=None))'''\n",
        "    block.add(Conv2D(out_channels, filter_dim, strides=(1,1), padding='same', activation=None))\n",
        "    block.add(BatchNormalization())\n",
        "    block.add(LeakyReLU(alpha=0.1))\n",
        "    return block"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qh0-r123P7Aw"
      },
      "source": [
        "def triple_conv(out_channels, in_channels):\n",
        "    block = Sequential()\n",
        "    block.add(conv_block(out_channels, 3))\n",
        "\n",
        "    block.add(conv_block(in_channels, filter_dim=1))\n",
        "    '''block.add(Conv2DTranspose(in_channels,3, strides=(1,1), padding='valid', activation=None))\n",
        "    block.add(BatchNormalization())\n",
        "    block.add(LeakyReLU(alpha=0.1))'''\n",
        "\n",
        "    block.add(conv_block(out_channels, 3))\n",
        "    return block"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4rOhSxGP-Yc"
      },
      "source": [
        "def darkCovidNet():\n",
        "    model = Sequential()\n",
        "    model.add(Input(shape=(input_size,input_size, 3)))\n",
        "    model.add(conv_block(8, 3))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2), strides=2))\n",
        "    model.add(conv_block(16, 3))\n",
        "    model.add(MaxPooling2D())\n",
        "    model.add(triple_conv(32, 16))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2), strides=2))\n",
        "    model.add(triple_conv(64, 32))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2), strides=2))\n",
        "    model.add(triple_conv(128, 64))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2), strides=2))\n",
        "    model.add(triple_conv(256, 128))\n",
        "    model.add(conv_block(128, filter_dim=1))\n",
        "    model.add(conv_block(256, 3))\n",
        "    model.add(conv_block(3, 3))\n",
        "    model.add(Flatten())\n",
        "    #model.add(Dense(192))\n",
        "    model.add(Dense(3))\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8m3Rf8mJFsW"
      },
      "source": [
        "def PKNet():\n",
        "    model = Sequential()\n",
        "    model.add(InputLayer(input_shape=(input_size , input_size , 3)))\n",
        "\n",
        "    # 1st conv block\n",
        "\n",
        "    model.add(Conv2D(8, (3, 3), activation='relu', strides=(1, 1), padding='same'))\n",
        "    model.add(MaxPool2D(pool_size=(2, 2), padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    # 2nd conv block\n",
        "\n",
        "    model.add(Conv2D(16, (3, 3), activation='relu', strides=(1, 1), padding='same'))\n",
        "    model.add(MaxPool2D(pool_size=(2, 2), padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    # 3rd conv block\n",
        "\n",
        "    model.add(Conv2D(32, (3, 3), activation='relu', strides=(1, 1), padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    # 4th conv block\n",
        "\n",
        "    model.add(Conv2D(16, (3, 3), activation='relu', strides=(1,1), padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    # 5th conv block\n",
        "\n",
        "    model.add(Conv2D(32, (3, 3), activation='relu', strides=(1,1), padding='same'))\n",
        "    model.add(MaxPool2D(pool_size=(2, 2), padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    # 6th conv block\n",
        "\n",
        "    model.add(Conv2D(64, (3, 3), activation='relu', strides=(1,1), padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    # 7th conv block\n",
        "\n",
        "    model.add(Conv2D(32, (3, 3), activation='relu', strides=(1,1), padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    # 8th conv block\n",
        "\n",
        "    model.add(Conv2D(64, (3, 3), activation='relu', strides=(1,1), padding='same'))\n",
        "    model.add(MaxPool2D(pool_size=(2, 2), padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    # 9th conv block\n",
        "\n",
        "    model.add(Conv2D(128, (3, 3), activation='relu', strides=(1,1), padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    # 10th conv bock\n",
        "\n",
        "    model.add(Conv2D(64, (3, 3), activation='relu', strides=(1,1), padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    # ANN block\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(units=100, activation='relu'))\n",
        "    model.add(Dense(units=100, activation='relu'))\n",
        "    model.add(Dropout(0.25))\n",
        "\n",
        "    # output layer\n",
        "\n",
        "    model.add(Dense(units=3, activation='softmax'))\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "keBWpl6qcjzt"
      },
      "source": [
        "def getTrainDataLoaders():\n",
        "    # Data augmentation and normalization for training\n",
        "    # Just normalization for validation\n",
        "    data_transforms = {\n",
        "        'train': transforms.Compose([\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ]),\n",
        "        'val': transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "    }\n",
        "\n",
        "    print(\"Initializing Datasets and Dataloaders...\")\n",
        "\n",
        "    # Create training and validation datasets\n",
        "    image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']}\n",
        "    # Create training and validation dataloaders\n",
        "    idx2label_dict = {v: k for k, v in image_datasets['train'].class_to_idx.items()}\n",
        "    dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'val']}\n",
        "\n",
        "    return idx2label_dict,dataloaders_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FsRrU9d-ihL"
      },
      "source": [
        "def saveInFile(features,labels,file_name):\n",
        "    saveData = np.concatenate((features,np.asarray([labels]).T),axis=1)\n",
        "    np.savetxt(file_name,saveData, delimiter=',',fmt='%8.6f')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0whbsVLddMEN"
      },
      "source": [
        "def train_model(model,tfmodel,tfmodel2, dataloaders, soft_mx, num_epochs=25):\n",
        "\n",
        "    since = time.time()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        #for phase in ['val']:\n",
        "        for phase in ['train', 'val']:\n",
        "            model.eval()\n",
        "\n",
        "            #-------1-------\n",
        "            y_true = np.asarray([],dtype=np.int32)\n",
        "            y_pred_aid = np.asarray([],dtype=np.int32)\n",
        "            #-------2-------\n",
        "            y_pred_dark = np.asarray([],dtype=np.int32)\n",
        "            #-------3-------\n",
        "            y_pred_pknet = np.asarray([],dtype=np.int32)\n",
        "            #---------------\n",
        "            \n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                #-------1-------\n",
        "                outputs = model(inputs)\n",
        "                outputs = soft_mx(outputs)\n",
        "                outputs = outputs.detach().cpu().numpy()\n",
        "                if y_pred_aid.shape[0] == 0:\n",
        "                    y_pred_aid =  outputs\n",
        "                else:\n",
        "                    y_pred_aid = np.append(y_pred_aid, outputs, axis=0)\n",
        "                #--------2------\n",
        "                tfinputs = inputs.data.detach().cpu().numpy()\n",
        "                tfinputs = np.transpose(tfinputs,(0,2,3,1))\n",
        "                outputs = tfmodel(tfinputs)\n",
        "                outputs = tf.keras.activations.softmax(outputs).numpy()\n",
        "                #print(\"1tf:\",outputs.shape)\n",
        "                if y_pred_dark.shape[0] == 0:\n",
        "                    y_pred_dark =  outputs\n",
        "                else:\n",
        "                    y_pred_dark = np.append(y_pred_dark, outputs, axis=0)\n",
        "                #--------3------\n",
        "                outputs = tfmodel2(tfinputs)\n",
        "                outputs = tf.keras.activations.softmax(outputs).numpy()\n",
        "                #print(\"2tf:\",outputs.shape)\n",
        "                if y_pred_pknet.shape[0] == 0:\n",
        "                    y_pred_pknet =  outputs\n",
        "                else:\n",
        "                    y_pred_pknet = np.append(y_pred_pknet, outputs, axis=0)\n",
        "                #---------------\n",
        "\n",
        "                y_true = np.append(y_true,labels.data.detach().cpu().numpy())\n",
        "                #break\n",
        "\n",
        "            saveInFile(y_pred_aid,y_true,'dense_out_'+phase+'.csv')\n",
        "            #print(\"1shape:\",y_pred_dark.shape,y_true.shape)\n",
        "            saveInFile(y_pred_dark,y_true,'dark_out_'+phase+'.csv')\n",
        "            #print(\"2shape:\",y_pred_pknet.shape,y_true.shape)\n",
        "            saveInFile(y_pred_pknet,y_true,'pknet_out_'+phase+'.csv')\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    #saveInFile(y_pred,y_true,'dense_out1.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LINfE0BCbMvW"
      },
      "source": [
        "model1 = CovidAID()\n",
        "model1.load_state_dict( torch.load(modelSave,map_location=torch.device('cpu')))\n",
        "model1 = model1.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1n0QlRaQK3u"
      },
      "source": [
        "model2 = darkCovidNet()\n",
        "model2.load_weights('CovidModel.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvec_lMeJSoL"
      },
      "source": [
        "model3 = PKNet()\n",
        "model3.load_weights('PKModel.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYW5q334cB9O"
      },
      "source": [
        "#print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0pe2PazcscG",
        "outputId": "5a05888b-ec0d-4874-e077-49a93ca67343",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "idx2label_dict,dataloaders = getTrainDataLoaders()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initializing Datasets and Dataloaders...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QntAtVSiczQk",
        "outputId": "10aa40c8-e82e-4d68-9366-44db27a9a99e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Observe that all parameters are being optimized\n",
        "soft_mx = nn.Softmax(dim=1)\n",
        "train_model(model1,model2,model3, dataloaders, soft_mx, num_epochs=num_epochs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/0\n",
            "----------\n",
            "\n",
            "Training complete in 0m 4s\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}